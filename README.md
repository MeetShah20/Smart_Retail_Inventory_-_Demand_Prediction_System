# Smart Retail Inventory - Demand Prediction System

[![Language](https://img.shields.io/badge/Language-Python-blue.svg)](https://www.python.org/)
[![Framework](https://img.shields.io/badge/ML%20Framework-TensorFlow/Keras-orange.svg)]() <!-- Or PyTorch -->
[![API](https://img.shields.io/badge/API-Flask/FastAPI-lightgrey.svg)]() <!-- Adjust -->
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) <!-- Replace -->

This microservice provides demand prediction capabilities for the **QuickCommerce** platform. It utilizes an LSTM (Long Short-Term Memory) neural network to forecast future product demand based on historical sales patterns and other relevant features.

## ðŸŽ¯ Objective

To accurately predict the quantity of specific products likely to be sold within a defined future timeframe, enabling better inventory management and reducing stockouts or overstock situations.

## ðŸ§  Model & Methodology

*   **Model Type:** Recurrent Neural Network (RNN) - specifically LSTM.
*   **Core Task:** Time Series Forecasting.
*   **Input Features:**
    *   Historical sales data (mandatory).
    *   Potential auxiliary features: seasonality indicators, promotion flags, day of the week, product category information.
*   **Output:**
    *   Predicted sales quantity for a future period (e.g., next 7 days).
    *   Optionally, a confidence score or prediction interval.

## ðŸ›  Technology Stack

| Category              | Technology / Library                            | Description                                 |
| :-------------------- | :---------------------------------------------- | :------------------------------------------ |
| **Language**          | Python 3.x                                      | Core programming language                   |
| **ML Framework**      | [Specify: TensorFlow/Keras or PyTorch]          | Building and training the LSTM model        |
| **API Framework**     | [Specify: Flask or FastAPI]                     | Serving predictions via a REST API          |
| **Data Handling**     | Pandas, NumPy                                   | Data manipulation and numerical operations  |
| **Training Data Proc.**| Apache Spark / PySpark                          | Processing large historical datasets (batch) |

*(**Note:** Fill in the `[Specify: ...]` placeholders.)*

## ðŸ“Š Data Sources

*   **Training Data:** Primarily historical sales records, likely sourced periodically from the main application's PostgreSQL database (`sales` or derived from `order_items`).
*   **Prediction Input:** A specific `productId`, a relevant segment of its recent historical data, and potentially real-time features (e.g., ongoing promotions).

## ðŸ”Œ API Endpoint

This service exposes a RESTful endpoint for on-demand predictions, typically consumed by the main backend service.

http
POST /api/v1/predict
Content-Type: application/json

{
  "productId": "string",           // ID of the product to forecast
  "historicalData": [            // Array of historical data points (e.g., daily sales)
    {"date": "YYYY-MM-DD", "quantity": number, ...},
    ...
  ],
  "features": {                  // Optional: Additional current features
    "seasonality": "string",     // e.g., "peak", "off-peak"
    "promotion_active": boolean
  },
  "forecast_horizon": number     // e.g., 7 for a 7-day forecast
}


**Success Response (Example):**

http
HTTP/1.1 200 OK
Content-Type: application/json

{
  "productId": "prod123",
  "predicted_total_quantity": 55, // Sum over the forecast horizon
  "forecast_range_start": "YYYY-MM-DD",
  "forecast_range_end": "YYYY-MM-DD",
  "confidence_score": 0.88,     // Optional measure of certainty
  "daily_predictions": [        // Optional: Day-by-day breakdown
     {"date": "YYYY-MM-DD", "predicted_quantity": number},
     ...
  ]
}

*(Adjust the request/response structure based on your precise implementation.)*

## âš™ Local Setup & Running (API Server)

1.  **Clone the repository:**
    bash
    git clone [URL_OF_THIS_DEMAND_PREDICTION_REPO]
    cd Smart_Retail_Inventory_-_Demand_Prediction_System
    
2.  **Create and Activate Python Virtual Environment:**
    bash
    python -m venv venv
    # Linux/macOS
    source venv/bin/activate
    # Windows (Command Prompt)
    # venv\Scripts\activate.bat
    # Windows (PowerShell)
    # venv\Scripts\Activate.ps1
    
3.  **Install Dependencies:**
    bash
    pip install -r requirements.txt
    
4.  **Load Pre-trained Model:**
    Ensure the trained model file(s) (e.g., `.h5`, `.pth`, `.pb`) are located in the designated directory (e.g., `models/`) as expected by the application code. These files are typically generated by the separate training pipeline.

5.  **Run the API Server:**
    *   **Using Flask:**
        bash
        # Ensure your main Flask app file is correctly named (e.g., app.py)
        export FLASK_APP=app.py # Linux/macOS
        # set FLASK_APP=app.py # Windows
        export FLASK_ENV=development # Optional: enables debug mode
        flask run --host=0.0.0.0 --port=5001 # Adjust port if needed
        
    *   **Using FastAPI (with Uvicorn):**
        bash
        # Ensure your main FastAPI app instance is correctly defined (e.g., app:app in app.py)
        uvicorn app:app --host 0.0.0.0 --port 5001 --reload # --reload for development
        
    > The API server will listen for requests on the specified host and port (e.g., `http://localhost:5001`).

## ðŸ“ˆ Model Training Pipeline

> **Note:** Model training is a separate, offline process, not part of running the API server.

*   Training typically involves processing large volumes of historical sales data using **Apache Spark**.
*   Scripts or notebooks (e.g., `train_model.py`, `training.ipynb`) located likely in a `training/` directory handle data loading, feature engineering, model training (LSTM), evaluation, and saving the model artifacts.
*   This pipeline should be run periodically (e.g., weekly, daily) using a scheduler (like `cron`, Apache Airflow) to keep the prediction model updated with the latest data trends.
